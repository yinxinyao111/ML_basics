{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cce9585-f5a1-459f-a1e2-6c0843e9afb3",
   "metadata": {},
   "source": [
    "# Transformer model (section 1: input embeddings)\n",
    "\n",
    "Convert original sentence into a vector of 512 dimensions\n",
    "\n",
    "original sentence -> input IDs (numbers that correspond to the position of each word inside the vocab) -> embeddings (vec size 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc03bb6-15dd-40ca-a580-7f2ec4db745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d93e322-2e4d-45c1-aa31-161798200e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int, # what is the dimension of the model\n",
    "                 vocab_size: int # how many words in the vocabulary\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ad8c87-b417-4e77-b7d6-239bc4dc53c9",
   "metadata": {},
   "source": [
    "1. Why multiple $\\sqrt{d_{model}}$ in the forward method for the embedding layer?\n",
    "- In the embedding layer, we multiply those weights by $\\sqrt{d_{model}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2175b24-4fb4-49f0-89d1-cd9d314e04af",
   "metadata": {},
   "source": [
    "# Section 2: positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9010e9ec-2272-437f-8def-a93be42a2fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int, \n",
    "                 seq_len: int, # maximum length of the sentence (create 1 positional vector for each position)\n",
    "                 dropout: float # makes the model less overfit\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Positional encoding is a matrix of shape seq_len * d_model (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        \n",
    "        # Create a vector of shape (seq_len, 1)\n",
    "        position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) # tensor shape: (seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply the sin to even positions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add batch dimension\n",
    "        pe = pe.unsqueece(0) # will become a tensor of shape (1, seq_len, d_model)\n",
    "\n",
    "        # To keep tensor in a model not as a learnt parameter, but save it when save model file, should register as buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to every word in the sentence\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # not learnt in the training process, this part of tensor not learnt\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd02df6a-9f8b-4a29-8ec3-dc8bdc7e62d1",
   "metadata": {},
   "source": [
    "For even positions: $PE(pos, 2i) = \\sin \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right)$\n",
    "\n",
    "For odd positions: $PE(pos, 2i+1) = \\cos \\left (\\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c810cafb-4e0d-4d48-802a-f98a90a71214",
   "metadata": {},
   "source": [
    "# Section 3: add and norm\n",
    "\n",
    "layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30504cac-549e-4c87-a93a-041a46297467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps: float = 10**-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # 2 parameters needed for layer normalization: alpha (will be multiplied), bias (will be added)\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True) # keepdim: usually the mean cancels out the dimension to which it is applied\n",
    "        std = x.std(dim = -1, keepdim = True)\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83716a02-445e-415c-82ab-709eb8790198",
   "metadata": {},
   "source": [
    "# Section 4: Feed forward layer\n",
    "\n",
    "A fully connected layer that our model uses in the encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96690582-2f40-4ccf-8053-07569729a0dd",
   "metadata": {},
   "source": [
    "$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$\n",
    "\n",
    "Two matrices ($W_1$ and $W_2$), ReLU in between, with a bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af3dd1b6-1397-44c5-a435-98edeecc815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 d_ff: int,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "        self.Linear_1 = nn.Linear(d_model, d_ff) # W1 and B1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.Linear_2  = nn.Linear(d_ff, d_model) # W2 and B2\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, d_ff) -> (batch, seq_len, d_model) finally apply linear to get d_model\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe5a175-e9d3-4f08-a98d-edaf6c56937a",
   "metadata": {},
   "source": [
    "# Section 5: Multihead attention\n",
    "\n",
    "In the encoder, the multihead attention takes the input of the encoder and uses it 3 times, query, key, values\n",
    "\n",
    "<img src=\"multihead_attention.png\" width=\"900\">\n",
    "\n",
    "$d_k = \\frac{d_{model}}{h}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5af5d4-7c3a-45be-a038-721e094f1457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "        # define the matrices by which we will multiply the query, key, and values, and output matrix Wo\n",
    "        self.w_q = nn.Linear(d_model, d_model) # Because in the above graph, Wq's shape is (d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model) #Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model) # Wv\n",
    "\n",
    "        self.w_o = nn.Linear(d_modelm d_model) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    @staticmethod # means we can call the \"attention\" function without having an instance of the \"MultiHeadAttention\" class\n",
    "    def attention(query)\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        # mask: prevent some words to interact with other words\n",
    "        query = self.w_q(q) # q' in the slides, (batch, seq_len, d_model) --> (batch. seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch. seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch. seq_len, d_model)\n",
    "\n",
    "        # now we want to divide the query, key, value matrices into smaller matrices so we can give give each small matrix different head\n",
    "        # using view method, keep batch dimension, we dont split the sentence, we split the embedding\n",
    "        # split the d_model into h by dk\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self,d_k).transpose(1, 2)\n",
    "        # why transpose? because we want h to be the 2nd dimension \n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, h, d_k) -> (batch, h, seq_len, d_k)\n",
    "        key = query.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], view.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e68a166-b660-4c2c-8cfa-9199e96a18ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
